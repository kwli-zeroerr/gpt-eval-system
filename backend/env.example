#############
# OpenAI / LLM
#############
OPENAI_API_KEY=your_key_here
# Optional overrides:
# OPENAI_BASE_URL=https://api.openai.com/v1
# OPENAI_MODEL=gpt-4o-mini

#############
# MinIO API Configuration (S3 Compatible)
#############
MINIO_ENDPOINT=localhost:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=zero0000
MINIO_BUCKET_NAME=knowledge
MINIO_SECURE=false

#############
# RagFlow API Configuration
#############
RAGFLOW_API_URL=http://your-ragflow-api-url:port
RAGFLOW_API_KEY=your-ragflow-api-key
RAGFLOW_DATASETS_JSON=datasets.json
RAGFLOW_TOP_K=5
RAGFLOW_SIMILARITY_THRESHOLD=0.0
RAGFLOW_VECTOR_SIMILARITY_WEIGHT=0.3
RAGFLOW_MAX_WORKERS=1
RAGFLOW_DELAY=0.5

#############
# Server Configuration
#############
# Backend server port (default: 8180, DO NOT use 8000)
PORT=8180

#############
# LLM Configuration
#############
# LLM API timeout in seconds (default: 120, increase if you see timeout errors)
LLM_TIMEOUT=120

#############
# Concurrency Configuration
#############
# Maximum number of processes for multiprocessing (default: min(4, cpu_count), min 2)
# 降低默认值以避免触发 rate limit
MAX_PROCESSES=3

# Maximum concurrent categories (default: 3, max 6)
# 控制同时处理的类别数量
MAX_CATEGORY_WORKERS=3

# Maximum workers per category (default: 3)
# 控制每个类别内的并发进程数
PER_CATEGORY_MAX_WORKERS=3

# Gunicorn workers for gpt-eval-system backend (default: CPU core count, minimum 4)
GUNICORN_WORKERS=8

#############
# LLM API Rate Limiting
#############
# Maximum concurrent LLM requests (default: 10)
# 控制全局最大并发 LLM 请求数
LLM_MAX_CONCURRENT=10

# Request interval in seconds (default: 0.1)
# 控制请求之间的最小间隔（秒）
LLM_REQUEST_INTERVAL=0.1

#############
# Evaluation Configuration
#############
# Maximum concurrent evaluation tasks (default: 5)
# 控制评测时的最大并发任务数，建议不超过LLM_MAX_CONCURRENT
# 如果有多个worker（如8个），可以设置为8以充分利用资源
# 注意：并发数过高可能导致API限流，建议根据实际情况调整
EVAL_MAX_CONCURRENT=8

#############
# Ragas Evaluation Configuration
#############
# Maximum tokens for Ragas evaluation (default: 128000, half of model's max context length 256000)
# 控制Ragas评测时的最大token数，用于处理长答案
# 设置为模型最大上下文长度（256000）的一半，确保即使输入tokens很大也不会超过限制
RAGAS_MAX_TOKENS=128000
